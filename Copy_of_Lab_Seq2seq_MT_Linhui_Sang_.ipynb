{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6840d517",
      "metadata": {
        "id": "6840d517"
      },
      "source": [
        "# Machine Translation with Seq2seq models via Pytorch\n",
        "\n",
        "The goal of this lab are to:\n",
        "- Familiarize yourself with the task of **Machine Translation (MT)**\n",
        "- Implement a basic **recurrent sequence-to-sequence** model in Pytorch\n",
        "- Train the model on a very simple English-French MT dataset\n",
        "- Implement an **attention** module into the model and visualize its results\n",
        "\n",
        "We will **for most of this lab** focus on the model and leave aside what are normally very important aspects of Machine Learning methodology: in particular, we won't use validation and test data to search for hyperparameter search. We will **split the data and retrain the model at the end of the lab** for a comparative performance evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c5a0974",
      "metadata": {
        "id": "6c5a0974"
      },
      "outputs": [],
      "source": [
        "# General stuff\n",
        "from io import open\n",
        "import re\n",
        "import unicodedata\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Nice printing\n",
        "from pprint import pprint\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Which device to use ?\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9fa8004",
      "metadata": {
        "id": "a9fa8004"
      },
      "source": [
        "### I Dataset and pre-processing\n",
        "\n",
        "We're going to work with data from the **tatoeba** website. This website proposes human-made translations for many (relatively) simple sentences, with sometimes several possible translations for one sentence.\n",
        "Pre-processed versions of the *tatoeba dataset* can be found on this [website](https://www.manythings.org/anki/). You are given the 'English $\\rightarrow$ French' data already cleaned, but you are free to use any other language you would prefer.  \n",
        "\n",
        "\n",
        "<div class='alert alert-block alert-warning'>\n",
        "            Question:</div>\n",
        "            \n",
        "We will define these as global variables - for convenience. Given what was said in class and how they are employed in this lab, explain briefly what each one is used for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30d2ecd3",
      "metadata": {
        "id": "30d2ecd3"
      },
      "outputs": [],
      "source": [
        "# Some global variables\n",
        "PAD_TOKEN = 0\n",
        "SOS_TOKEN = 1\n",
        "EOS_TOKEN = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fbcf156",
      "metadata": {
        "id": "3fbcf156"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "280f96ce",
      "metadata": {
        "id": "280f96ce"
      },
      "source": [
        "From the previous pytorch lab, we know we will require to define some parameters. We can already choose the maximum length of sequences, the size of our batches, and the internal dimension used by our model. Note that the length of sequence is rather short in this data - you can take a look at the histogram.\n",
        "\n",
        "**Put ```fra.txt``` in the current directory**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81270d1e",
      "metadata": {
        "id": "81270d1e"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "max_length = 10\n",
        "batch_size = 32\n",
        "hidden_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfb59d36",
      "metadata": {
        "id": "bfb59d36"
      },
      "outputs": [],
      "source": [
        "# Read the file and split into lines\n",
        "parallel = open('fra.txt', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba28e54b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba28e54b",
        "outputId": "41a93495-e184-4788-ed56-c1930abca259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & '\n",
            " '#1158250 (Wittydev)',\n",
            " 'Go.\\tMarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & '\n",
            " '#8090732 (Micsmithel)',\n",
            " 'Go.\\tEn route !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & '\n",
            " '#8267435 (felix63)',\n",
            " 'Go.\\tBouge !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & '\n",
            " '#9022935 (Micsmithel)',\n",
            " 'Hi.\\tSalut !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & '\n",
            " '#509819 (Aiji)']\n"
          ]
        }
      ],
      "source": [
        "# Data looks like this\n",
        "pprint(parallel[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff8de7fd",
      "metadata": {
        "id": "ff8de7fd"
      },
      "source": [
        "We will need to clean this up. Use the regular expression package ```re``` to remove any non letter character. Be careful, though, with French, you need to keep the accents. We will then organize the data into pairs, as is usual in MT.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f5e8a6d",
      "metadata": {
        "id": "2f5e8a6d"
      },
      "outputs": [],
      "source": [
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    # Convert to lowercase\n",
        "    s = s.lower().strip()\n",
        "\n",
        "    # Normalize unicode (decomposed form)\n",
        "    s = unicodedata.normalize('NFD', s)\n",
        "\n",
        "    # Keep only French letters, accents, apostrophes, and ligatures\n",
        "    s = re.sub(r\"[^a-zA-ZÀ-ÖØ-öø-ÿœæ']\", \" \", s)\n",
        "\n",
        "    # Normalize back to NFC (composed form)\n",
        "    s = unicodedata.normalize('NFC', s)\n",
        "\n",
        "    # Remove multiple spaces\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16663844",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16663844",
        "outputId": "d109baa8-1582-4419-b3c4-149da515b065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total valid pairs after metadata removal: 232736\n"
          ]
        }
      ],
      "source": [
        "# Split every line into pairs and normalize\n",
        "pairs = []\n",
        "for line in parallel:\n",
        "    parts = line.strip().split(\"\\t\")  # Split into parts\n",
        "    if len(parts) >= 2:  # Ensure at least two elements (English & French)\n",
        "        eng, fra = normalizeString(parts[0]), normalizeString(parts[1])  # Take only the first two columns\n",
        "        if eng and fra:  # Ensure they are not empty\n",
        "            pairs.append([eng, fra])\n",
        "\n",
        "print(f\"Total valid pairs after metadata removal: {len(pairs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49bd29eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49bd29eb",
        "outputId": "27bb4558-4d6f-49ed-86d8-6c8463d698e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['go', 'va'],\n",
            " ['go', 'marche'],\n",
            " ['go', 'en route'],\n",
            " ['go', 'bouge'],\n",
            " ['hi', 'salut']]\n"
          ]
        }
      ],
      "source": [
        "pprint(pairs[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da162c2",
      "metadata": {
        "id": "7da162c2"
      },
      "source": [
        "Begin with implementing a class ```Vocab``` that will accumulate counts and indexes of words into language-specific dictionnaries. In this case, we would like the vocabulary to be built on the fly, to work well with the format of our data (parallel sentences from both languages).\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01f09ee9",
      "metadata": {
        "id": "01f09ee9"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.word2count = {}\n",
        "        self.word2idx = {\"SOS\": SOS_TOKEN, \"EOS\": EOS_TOKEN}\n",
        "        self.idx2word = {SOS_TOKEN: \"SOS\", EOS_TOKEN: \"EOS\"}\n",
        "\n",
        "    # Implemented assuming we will process lines one by one, easier given the format of our data\n",
        "    def addSent(self, sent):\n",
        "        for word in sent.split():  # Tokenizing sentence\n",
        "            if word not in self.word2idx:\n",
        "                idx = len(self.word2idx)  # New word index\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "                self.word2count[word] = 1\n",
        "            else:\n",
        "                self.word2count[word] += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daa59369",
      "metadata": {
        "id": "daa59369"
      },
      "source": [
        "Then, create a function ```tensorFromSentence``` that will take an untokenized sentence (hence, a string), a ```Vocab``` object, and the ```max_length``` parameter as inputs, and return a ```LongTensor``` representing the sequence of indexes.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e6cb648",
      "metadata": {
        "id": "3e6cb648"
      },
      "outputs": [],
      "source": [
        "def tensorFromSentence(sent, vocab, max_length):\n",
        "    indexes = [vocab.word2idx.get(word, vocab.word2idx.get('<UNK>', 0)) for word in sent.split()]\n",
        "    indexes = [SOS_TOKEN] + indexes + [EOS_TOKEN]  # Add SOS and EOS tokens\n",
        "\n",
        "    if len(indexes) > max_length:\n",
        "        indexes = indexes[:max_length - 1] + [EOS_TOKEN]  # Ensure EOS is included\n",
        "\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b7045aa",
      "metadata": {
        "id": "6b7045aa"
      },
      "source": [
        "Finally, complete this ```TranslationDataset``` class inheriting from ```Dataset```. It should, from the list of parallel sentences:\n",
        "- Apply an optional filter to possibly reduce the dataset size and complexity,\n",
        "- Instantiate and build ```Vocab``` objects for both languages,\n",
        "- Create two lists containing ```LongTensor``` objects for each language,\n",
        "- Group them into two tensors of the appropriate size with ```pad_sequence```.\n",
        "\n",
        "You should note that, depending on the ordering of the pairs, one language will be the **source**, and the other will be the **target** of our model. In this case, English is the source and French the target.\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b70bd228",
      "metadata": {
        "id": "b70bd228"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, parallel_data, max_length = 10, filter_target_prefixes = None):\n",
        "        # We will select some subset on the data to avoid having too much\n",
        "        self.pairs = self.filterData(parallel_data, filter_target_prefixes)\n",
        "        # Creating both vocabularies\n",
        "        self.max_length = max_length\n",
        "        self.input_lang = Vocab() # English\n",
        "        self.output_lang = Vocab()  # French\n",
        "        # Filling both vocabularies\n",
        "        for pair in self.pairs:\n",
        "            self.input_lang.addSent(pair[0])  # English sentence\n",
        "            self.output_lang.addSent(pair[1])  # French sentence\n",
        "        # List of tensors to be created\n",
        "        self.tensor_inputs = [tensorFromSentence(pair[0], self.input_lang, max_length) for pair in self.pairs]\n",
        "        self.tensor_outputs = [tensorFromSentence(pair[1], self.output_lang, max_length) for pair in self.pairs]\n",
        "        # Put them all at the same size with pad_sequence\n",
        "        self.tensor_inputs = pad_sequence(self.tensor_inputs, batch_first=True, padding_value=PAD_TOKEN)\n",
        "        self.tensor_outputs = pad_sequence(self.tensor_outputs, batch_first=True, padding_value=PAD_TOKEN)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # The iterator just gets one particular example\n",
        "        # The dataloader will take care of the shuffling and batching\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        return self.tensor_inputs[idx], self.tensor_outputs[idx]\n",
        "\n",
        "    def filterPair(self, pair, prefixes):\n",
        "        return pair[0].startswith(prefixes)\n",
        "\n",
        "    def filterData(self, pairs, filter_target_prefixes):\n",
        "        if filter_target_prefixes is not None:\n",
        "            return [pair for pair in pairs if self.filterPair(pair, filter_target_prefixes)]\n",
        "        else:\n",
        "            return pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c0649e",
      "metadata": {
        "id": "e6c0649e"
      },
      "source": [
        "Create a ```TranslationDataset``` from our data, with no filter, and look at its size, and the sizes of the vocabularies. What could be a problem here ?\n",
        "\n",
        "<div class='alert alert-block alert-warning'>\n",
        "            Question:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c976dc33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c976dc33",
        "outputId": "a515cddb-9fa2-4aec-eb38-7c79cf84d298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 232736\n",
            "English vocabulary size: 16499\n",
            "French vocabulary size: 24596\n"
          ]
        }
      ],
      "source": [
        "# Create dataset with no filtering\n",
        "dataset = TranslationDataset(pairs, max_length=10, filter_target_prefixes=None)\n",
        "\n",
        "# Print dataset size and vocabulary sizes\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"English vocabulary size: {len(dataset.input_lang)}\")\n",
        "print(f\"French vocabulary size: {len(dataset.output_lang)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class='alert alert-block alert-warning'>\n",
        "            Answer:</div>\n",
        "1️⃣ Large Vocabulary Size\n",
        "\n",
        "The French vocabulary (11165) is significantly larger than the English vocabulary (6988).\n",
        "This can increase memory usage, slow down training, and make the model harder to generalize.\n",
        "Handling a large vocabulary can increase the complexity of the model. The embedding layers and the output layers of the decoder will have to handle a large number of classes, which can make the training process more challenging.\n",
        "\n",
        "2️⃣ Rare Words & Data Sparsity\n",
        "\n",
        "Many words might appear only once or twice, making learning inefficient.\n",
        "\n",
        "3️⃣ Sentence Length & Padding Issues\n",
        "\n",
        "The dataset uses a fixed max_length=10, but most sentences may be much shorter or longer.\n",
        "Padding short sentences wastes memory, while truncating long sentences loses information.\n",
        "\n",
        "4️⃣ Overfitting\n",
        "\n",
        "With a large dataset, there is a risk of overfitting, especially if the model is complex. Proper regularization techniques and validation are necessary to mitigate this risk."
      ],
      "metadata": {
        "id": "AcFHRdD0ReuD"
      },
      "id": "AcFHRdD0ReuD"
    },
    {
      "cell_type": "markdown",
      "id": "dc6b413c",
      "metadata": {
        "id": "dc6b413c"
      },
      "source": [
        "We will now use a filter: we will only consider pairs of sentences which English begins with chains of characters from the ```prefixes``` set.\n",
        "\n",
        "- Create the dataset with this filter.\n",
        "- Look at the sizes involved.\n",
        "- **Split the data and keep a small subset for testing** (done at the end of the lab)\n",
        "- Create a dataloader with the previously defined ```batch_size```.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8ef5801",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8ef5801",
        "outputId": "7ab35e72-f679-4ddc-e3e7-b37c10838eca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Dataset Size: 3544\n",
            "English Vocabulary Size: 2036\n",
            "French Vocabulary Size: 2693\n"
          ]
        }
      ],
      "source": [
        "# Consider only the sentences beginning with these\n",
        "prefixes = (\"i am \", \"i m \",\n",
        "            \"he is\", \"he s \",\n",
        "            \"she is\", \"she s \",\n",
        "            \"you are\", \"you re \",\n",
        "            \"we are\", \"we re \",\n",
        "            \"they are\", \"they re \")\n",
        "\n",
        "# New dataset:\n",
        "filtered_dataset = TranslationDataset(pairs, max_length=10, filter_target_prefixes=prefixes)\n",
        "\n",
        "# Look at the sizes involved\n",
        "print(f\"Filtered Dataset Size: {len(filtered_dataset)}\")\n",
        "print(f\"English Vocabulary Size: {len(filtered_dataset.input_lang)}\")\n",
        "print(f\"French Vocabulary Size: {len(filtered_dataset.output_lang)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define split sizes (e.g., 80% training, 20% testing)\n",
        "train_size = int(0.8 * len(filtered_dataset))\n",
        "test_size = len(filtered_dataset) - train_size\n",
        "\n",
        "# Perform the split\n",
        "train_dataset, test_dataset = random_split(filtered_dataset, [train_size, test_size])\n",
        "\n",
        "print(f\"Training Dataset Size: {len(train_dataset)}\")\n",
        "print(f\"Testing Dataset Size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcDLFsNlS2qT",
        "outputId": "9ad552ff-a11a-4cdc-83b8-bf25e2e0e075"
      },
      "id": "WcDLFsNlS2qT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Size: 2835\n",
            "Testing Dataset Size: 709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb041242",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb041242",
        "outputId": "ab35868e-3db5-45fc-f148-a10d5ae30a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Batch Shape: torch.Size([32, 10])\n",
            "Target Batch Shape: torch.Size([32, 10])\n"
          ]
        }
      ],
      "source": [
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create dataloaders\n",
        "training_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "testing_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Print sample batch shape\n",
        "for batch in training_dataloader:\n",
        "    input_batch, target_batch = batch\n",
        "    print(f\"Input Batch Shape: {input_batch.shape}\")\n",
        "    print(f\"Target Batch Shape: {target_batch.shape}\")\n",
        "    break  # Stop after first batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14f35763",
      "metadata": {
        "id": "14f35763"
      },
      "source": [
        "### II - Sequence to sequence architecture and training\n",
        "\n",
        "We will now create two pytorch objects, which will inherit from ```Module```: the ```EncoderRNN``` and the ```DecoderRNN``` classes. Both are based on RNNs; we will use the lighter ```GRU``` (gated recurrent unit) recurrent layer.\n",
        "While we won't check it with validation data, we should try to avoid overfitting with ```Dropout```."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61234a32",
      "metadata": {
        "id": "61234a32"
      },
      "source": [
        "Begin by completing the **encoder**. It uses an ```Embedding``` layer, which has as many vectors as the size of the **source** vocabularies, plus the ```GRU```. Both embeddings and the recurrent layer use dimension ```hidden_size```. It should output two things:\n",
        "- A sequence of vectors, corresponding to the representations of each input word that has gone through the encoder,\n",
        "- The last hidden state used by the GRU of the encoder.\n",
        "\n",
        "**Important**: with our first decoder, we will only use the **last hidden state**. However, we can still add the sequence of representations to the outputs, as we will need it for the *attention module* later.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef96860",
      "metadata": {
        "id": "0ef96860"
      },
      "outputs": [],
      "source": [
        "# Create encoder\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, dropout=dropout_p, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))  # Apply embedding and dropout\n",
        "        output, hidden = self.gru(embedded) # Forward pass through GRU\n",
        "        # Sum the forward and backward hidden states for better representation\n",
        "        hidden = hidden.view(2, -1, self.hidden_size)\n",
        "        hidden = hidden.sum(dim=0, keepdim=True)\n",
        "\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f1e1ac",
      "metadata": {
        "id": "28f1e1ac"
      },
      "source": [
        "Next, you will need to complete the **decoder**. Besides the ```Embedding``` (for the **target** language) and ```GRU```, it needs an additional layer: a ```Linear``` layer to obtain output scores for the next word to be predicted.\n",
        "The ```forward``` function is however a little more complicated: we will need it to be able to re-use what was predicted at the previous step during inference. Therefore, we will use the old-fashioned way: a **loop**. To summarize, we will:\n",
        "- Create an empty tensor containing only the first token of the output sequence (*which is ?*) with ```torch.empty```.\n",
        "- If we are in training mode, we can fill out that tensor with what we know to be the rest of the output sequence, make it go through the recurrent layer, and obtain scores.\n",
        "- If we are in inference mode, we need to make a prediction at each step to re-insert the corresponding index as input afterwards. We can use the ```topk``` method to get the best index directly ! **Important:** use the ```detach()``` method to cut this from the computational graph.  \n",
        "\n",
        "In both cases, we loop through the sequence and apply the same operations, which are in ```forward_step```. We return the log-probabilities of prediction at each step.\n",
        "\n",
        "**Important:** Again, we also return an empty placeholder variable which we will later use for attention.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f67a9b50",
      "metadata": {
        "id": "f67a9b50"
      },
      "outputs": [],
      "source": [
        "# Create decoder\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, max_length, target_tensor=None):\n",
        "        # We build the  decoder the old school way:\n",
        "        # As we will need to loop through the decoder for inference, let's use the same structure for training\n",
        "        # We will process one input and predict one output at the time, with a method implementing the recurrent step: \"foward_step\"\n",
        "        batch_size = encoder_hidden.size(1)\n",
        "        # Create the input to the decoder: which token is it ? Put it in \"fill_\"\n",
        "        # Which shape should it be ?\n",
        "        decoder_input = torch.empty((batch_size, 1), dtype=torch.long, device=device).fill_(SOS_TOKEN)\n",
        "        # Where does the first hidden state come from ?\n",
        "        decoder_hidden = encoder_hidden\n",
        "        # We'll keep the output in a list\n",
        "        decoder_outputs = []\n",
        "\n",
        "        # Looping on the output sequence\n",
        "        for i in range(max_length):\n",
        "            # Apply the forward_step function ...\n",
        "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
        "            # and keep the output\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            # We are in training mode: we know the target\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                # Which shape do we need ?\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
        "\n",
        "            # We are doing inference, we need to predict the next word and re-use it as input\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                # Use the topk function to get the best index\n",
        "                topv, topi = decoder_output.topk(1)  # Get top word index\n",
        "                # Very important: to be re-used as input, detach from computational graph\n",
        "                # Which shape do we need ?\n",
        "                decoder_input = topi.squeeze(-1).detach().unsqueeze(1)\n",
        "\n",
        "        # Concatenate outputs on the second dimension (length of the sequence)\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        # Apply log_softmax\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        # We return `None` for consistency in the training loop - it will be used for attention later\n",
        "        return decoder_outputs, decoder_hidden, None\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        # Get your input through embedding, an activation function, the recurrent layer, and the output layer\n",
        "        embedded = self.embedding(input)  # Convert input indices to word embeddings\n",
        "        output, hidden = self.gru(embedded, hidden)  # Pass through GRU\n",
        "        output = self.out(output) # Predict next word probabilities\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f0d850",
      "metadata": {
        "id": "58f0d850"
      },
      "source": [
        "Create an instance of one ```EncoderRNN``` and one ```DecoderRNN```. In order to do this, get the vocabulary sizes for the appropriate languages from the ```TranslationDataset``` object.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295612e6",
      "metadata": {
        "id": "295612e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "361589c0-79a4-46a2-929f-a0e6fe7ca661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Get vocabulary sizes from TranslationDataset\n",
        "input_vocab_size = len(filtered_dataset.input_lang)  # English vocabulary size\n",
        "output_vocab_size = len(filtered_dataset.output_lang)  # French vocabulary size\n",
        "hidden_size = 128  # Choose a hidden size for embeddings and GRU\n",
        "\n",
        "encoder = EncoderRNN(input_vocab_size, hidden_size).to(device)\n",
        "decoder = DecoderRNN(hidden_size, output_vocab_size).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3ae2b3e",
      "metadata": {
        "id": "f3ae2b3e"
      },
      "source": [
        "Implement the training loop into the ```train_epoch``` function. Follow the model from the previous lab. Note that we will use separated *optimizers* for the encoder and decoder. **Be careful to the sizes of the model outputs for use with the criterion !**\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec7ef47a",
      "metadata": {
        "id": "ec7ef47a"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "        input_tensor = input_tensor.to(device)\n",
        "        target_tensor = target_tensor.to(device)\n",
        "        # Initiate gradient\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "        # Forward\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor.size(1), target_tensor)\n",
        "        # Compute loss : put the output at the right size, the reference too, and apply the criterion\n",
        "        decoder_outputs = decoder_outputs.view(-1, decoder_outputs.size(-1))  # Reshape to (batch_size * max_length, vocab_size)\n",
        "        target_tensor = target_tensor.view(-1)  # Reshape to (batch_size * max_length)\n",
        "        loss = criterion(decoder_outputs, target_tensor)  # Apply loss function\n",
        "        # Compute gradient\n",
        "        loss.backward()\n",
        "        # Update weights\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "        # Keep track of loss\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5324307",
      "metadata": {
        "id": "f5324307"
      },
      "source": [
        "We can know simply loop on this using the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0a14bc",
      "metadata": {
        "id": "3c0a14bc"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs=80, learning_rate=0.001, print_every=10, plot_every=10):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "    # Initialize optimizers\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    # Initialize criterion\n",
        "    criterion = nn.NLLLoss()\n",
        "    # Training loop\n",
        "    for epoch in range(n_epochs):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if (epoch + 1) % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('(%d %d%%) %.4f' % (epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if (epoch + 1) % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6638f457",
      "metadata": {
        "id": "6638f457"
      },
      "source": [
        "And we need to also implement an ```evaluate``` function. Here, we will need to use the decoder in **inference** node, so it will re-use what output it generates to continue processing. We will then transform this sequence of outputs into **words**. What is the stopping condition for our model generating words ?\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bb3e0cf",
      "metadata": {
        "id": "9bb3e0cf"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length, input_lang, output_lang):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    # One example to evaluate\n",
        "    # We need to make it into a batch of one exemple to respect tensor dimensions\n",
        "    input_tensor = tensorFromSentence(sentence, input_lang, max_length).view(1, -1).to(device)\n",
        "    # Forward\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "    decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden, max_length)\n",
        "    # Get best output\n",
        "    topv, topi = decoder_outputs.topk(1)  # Get top predicted word indices\n",
        "    topi = topi.squeeze()  # Remove extra dimensions\n",
        "    # Decode until stopping condition ?\n",
        "    decoded_words = []\n",
        "    for i in range(max_length):\n",
        "        if topi[i].item() == EOS_TOKEN:  # Stop if EOS is predicted\n",
        "            break\n",
        "        decoded_words.append(output_lang.idx2word[topi[i].item()])  # Convert index to word\n",
        "\n",
        "    return decoded_words, decoder_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23ebafa5",
      "metadata": {
        "id": "23ebafa5"
      },
      "source": [
        "Let's use this function to evaluate our model on a random subset of the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3236f826",
      "metadata": {
        "id": "3236f826"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, dataset, n=10):\n",
        "    # do n examples\n",
        "    for i in range(n):\n",
        "        # select one from the known data to avoid vocabulary issue\n",
        "        pair = random.choice(dataset.pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], dataset.max_length, dataset.input_lang, dataset.output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d57249c",
      "metadata": {
        "id": "3d57249c"
      },
      "source": [
        "Now, execute the training loop for, and look at what it generates. It should be fast on a cpu, and not take too long on a GPU.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a6ea9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "f3a6ea9f",
        "outputId": "0315817e-d174-461f-fbb2-4a2126c74898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4 5%) 2.9029\n",
            "(9 11%) 1.6889\n",
            "(14 17%) 1.1150\n",
            "(19 23%) 0.7405\n",
            "(24 30%) 0.4948\n",
            "(29 36%) 0.3341\n",
            "(34 42%) 0.2332\n",
            "(39 48%) 0.1695\n",
            "(44 55%) 0.1304\n",
            "(49 61%) 0.1065\n",
            "(54 67%) 0.0920\n",
            "(59 73%) 0.0821\n",
            "(64 80%) 0.0757\n",
            "(69 86%) 0.0702\n",
            "(74 92%) 0.0694\n",
            "(79 98%) 0.0642\n",
            "> they are christians\n",
            "= ce sont des chre tiennes\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "GRU: Expected input to be 2D or 3D, got 4D instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-315d4c841925>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluateRandomly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-d05d9c3e9216>\u001b[0m in \u001b[0;36mevaluateRandomly\u001b[0;34m(encoder, decoder, dataset, n)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutput_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moutput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-6c504b0521cf>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, sentence, max_length, input_lang, output_lang)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Get best output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get top predicted word indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-856c1a240b64>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_outputs, encoder_hidden, max_length, target_tensor)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# Apply the forward_step function ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;31m# and keep the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-856c1a240b64>\u001b[0m in \u001b[0;36mforward_step\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Get your input through embedding, an activation function, the recurrent layer, and the output layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert input indices to word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass through GRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Predict next word probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1353\u001b[0m             \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1355\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1356\u001b[0m                     \u001b[0;34mf\"GRU: Expected input to be 2D or 3D, got {input.dim()}D instead\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: GRU: Expected input to be 2D or 3D, got 4D instead"
          ]
        }
      ],
      "source": [
        "train(training_dataloader, encoder, decoder, print_every=5, plot_every=5)\n",
        "evaluateRandomly(encoder, decoder, filtered_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4183c681",
      "metadata": {
        "id": "4183c681"
      },
      "source": [
        "### III - Attention module\n",
        "\n",
        "We will know implement a new class ```Attention``` inheriting from ```Module```.\n",
        "\n",
        "Begin by implementing it following the scheme presented in class. In order to implement this efficiently, you will need to use *batched* operations and pay attention to shapes. in particular, use the **batched matrix multiplication** ```torch.bmm```. Use shape manipulation functions (```permute, squeeze, unsqueeze```) when needed.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba6303c2",
      "metadata": {
        "id": "ba6303c2"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        # What shape do we need the query in ?\n",
        "        query = query.permute(0, 2, 1)\n",
        "        # Compute similarity scores with bmm. Any shape change for keys ?\n",
        "        scores = torch.bmm(keys, query)\n",
        "        # Apply softmax to get weights. Any shape change for scores ?\n",
        "        weights = F.softmax(scores, dim=1)\n",
        "        # Use bmm to make weighted sum. Any shape change required ?\n",
        "        context = torch.bmm(weights.permute(0, 2, 1), keys)\n",
        "        return context, weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ca8cb3c",
      "metadata": {
        "id": "3ca8cb3c"
      },
      "source": [
        "Then, you will need to modify the decoder class into a new ```AttentionDecoderRNN```. The usual way of implementing the loop in ```forward_step``` is as follows:\n",
        "\n",
        "- Apply the recurrent loop as before: $\\mathbf{s}_{t} = \\text{GRU}(\\mathbf{r}_t, \\mathbf{s}_{t-1})$\n",
        "- Noting $\\mathbf{z}_t = Attention(\\mathbf{H}, \\mathbf{s}_t)$ the output of the attention, we compute a modified state $\\tilde{s}_t$: $$ \\tilde{s}_t = tanh(\\mathbf{W}_a \\times [\\mathbf{z}_t; \\mathbf{s}_t])$$ based on the concatenation of the attention output and output of the GRU.\n",
        "- We predict score based on this modified new state: $\\mathbf{o}_t = \\mathbf{W}_{out} \\times \\tilde{s}_t$.\n",
        "\n",
        "**Important**:\n",
        "- You need to instantiate the ```Attention``` class when building the decoder.\n",
        "- You also need a new parameter representing $\\mathbf{W}_a$, of the appropriate size - as this matrix is applied to a concatenation of the attention output and the decoder hidden state.\n",
        "- You need to keep track of attention weights at each step, and also concatenate them and output them at the end of the ```forward```.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f451debc",
      "metadata": {
        "id": "f451debc"
      },
      "outputs": [],
      "source": [
        "class AttentionDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttentionDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        # Don't forget to instantiate the Attention\n",
        "        self.attention = Attention(hidden_size)\n",
        "        # And the new linear layer needed\n",
        "        self.W_a = nn.Linear(2 * hidden_size, hidden_size)  # Applies W_a × [z_t ; s_t]\n",
        "        # Final output layer\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        # Get your input through embedding, apply the recurrent layer\n",
        "        embedded = self.dropout(self.embedding(input))  # (batch_size, 1, hidden_size)\n",
        "        if embedded.dim() == 4:\n",
        "            embedded = embedded.squeeze(1)\n",
        "\n",
        "        output, hidden = self.gru(embedded, hidden)  # Apply GRU `output` shape: (batch_size, 1, hidden_size)\n",
        "        # Compute the attention\n",
        "        context, attn_weights = self.attention(output, encoder_outputs)  # (batch_size, 1, hidden_size), (batch_size, 1, seq_len)\n",
        "        if context.dim() == 4:\n",
        "            context = context.squeeze(1)\n",
        "        # Concatenate the result of the attention and the encoder outputs\n",
        "        combined = torch.cat((context, output), dim=2)  # Shape: (batch_size, 1, 2*hidden_size)\n",
        "\n",
        "        # Apply the linear transformation and tanh\n",
        "        s_tilde = torch.tanh(self.W_a(combined))  # Shape: (batch_size, 1, hidden_size)\n",
        "        # Apply the last layer to obtain scores\n",
        "        output = self.out(s_tilde)  # (batch_size, 1, output_size)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, max_length, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty((batch_size, 1), dtype=torch.long, device=device).fill_(SOS_TOKEN)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        # New: attention list\n",
        "        attention_weights_list = []\n",
        "\n",
        "        for i in range(max_length):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # and keep the output\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            # Also keep track of attentions\n",
        "            attention_weights_list.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
        "\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                topv, topi = decoder_output.topk(1)\n",
        "\n",
        "                # Very important: to be re-used as input, detach from computational graph\n",
        "                decoder_input = topi.squeeze(-1).detach()\n",
        "                decoder_input = decoder_input.unsqueeze(1)  # Ensure (batch_size, 1)\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        attentions = torch.cat(attention_weights_list, dim=1)  # (batch_size, max_length, seq_len)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1) # Apply log_softmax\n",
        "        return decoder_outputs, decoder_hidden, attentions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d1e4b35",
      "metadata": {
        "id": "3d1e4b35"
      },
      "source": [
        "Create new encoder and decoders instances for this model, and train them !\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c36774c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "c36774c7",
        "outputId": "5c2ed9e0-15b1-42fe-9606-12c6a338d45d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected size for first two dimensions of batch2 tensor to be: [32, 256] but got: [32, 128].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-51815bab6b00>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train the attention-based model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-96c7017e7ef9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, encoder, decoder, n_epochs, learning_rate, print_every, plot_every)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-121d64aaedd3>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Compute loss : put the output at the right size, the reference too, and apply the criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdecoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reshape to (batch_size * max_length, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-d986e60281ce>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_outputs, encoder_hidden, max_length, target_tensor)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;31m# and keep the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-d986e60281ce>\u001b[0m in \u001b[0;36mforward_step\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply GRU `output` shape: (batch_size, 1, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Compute the attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, 1, hidden_size), (batch_size, 1, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-6c6d0ac28711>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, keys)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Compute similarity scores with bmm. Any shape change for keys ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Apply softmax to get weights. Any shape change for scores ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [32, 256] but got: [32, 128]."
          ]
        }
      ],
      "source": [
        "# Define hidden size\n",
        "hidden_size = 128\n",
        "\n",
        "encoder_att = EncoderRNN(len(filtered_dataset.input_lang), hidden_size).to(device)\n",
        "decoder_att = AttentionDecoderRNN(hidden_size, len(filtered_dataset.output_lang)).to(device)\n",
        "\n",
        "# Train the attention-based model\n",
        "train(training_dataloader, encoder_att, decoder_att, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a75d6278",
      "metadata": {
        "id": "a75d6278"
      },
      "source": [
        "Use the following function to visualize the attention learnt by the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ffe06a3",
      "metadata": {
        "id": "7ffe06a3"
      },
      "outputs": [],
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.detach().cpu().numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "    plt.show()\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence, encoder, decoder, dataset):\n",
        "    output_words, attentions = evaluate(encoder, decoder, input_sentence, dataset.max_length, dataset.input_lang, dataset.output_lang)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c0ddb1f",
      "metadata": {
        "id": "0c0ddb1f"
      },
      "outputs": [],
      "source": [
        "evaluateRandomly(encoder_att, decoder_att, filtered_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c992013",
      "metadata": {
        "id": "3c992013"
      },
      "outputs": [],
      "source": [
        "evaluateAndShowAttention('i am not a doctor but a teacher', encoder_att, decoder_att, filtered_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03b0c870",
      "metadata": {
        "id": "03b0c870"
      },
      "source": [
        "We played here with a dataset but did not rigorously evaluate. The usual metric for Machine Translation is the [**BLEU Score**](https://aclanthology.org/P02-1040.pdf). You can find existing implementations, for example in [Huggingface](https://huggingface.co/spaces/evaluate-metric/bleu). Rigorously experiment with\n",
        "- A model with attention\n",
        "- A model without attention\n",
        "\n",
        "and use the BLEU score on the test set to compare them.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "696adf96",
      "metadata": {
        "id": "696adf96"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu(encoder, decoder, test_dataloader, dataset):\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        input_tensor, target_tensor = batch\n",
        "        input_sentences = [\n",
        "            [dataset.input_lang.idx2word[idx.item()] for idx in input_tensor[0] if idx.item() not in [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]]\n",
        "        ]\n",
        "        target_sentences = [\n",
        "            [dataset.output_lang.idx2word[idx.item()] for idx in target_tensor[0] if idx.item() not in [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]]\n",
        "        ]\n",
        "\n",
        "        output_words, _ = evaluate(encoder, decoder, \" \".join(input_sentences[0]), dataset.max_length, dataset.input_lang, dataset.output_lang)\n",
        "        hypotheses.append(output_words)\n",
        "        references.append(target_sentences)\n",
        "\n",
        "    return corpus_bleu(references, hypotheses)"
      ],
      "metadata": {
        "id": "tHuD-29F1tiG"
      },
      "id": "tHuD-29F1tiG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_no_att = compute_bleu(encoder, decoder, testing_dataloader, filtered_dataset)\n",
        "print(f\"BLEU Score (No Attention): {bleu_no_att:.4f}\")\n",
        "bleu_att = compute_bleu(encoder_att, decoder_att, testing_dataloader, filtered_dataset)\n",
        "print(f\"BLEU Score (With Attention): {bleu_att:.4f}\")"
      ],
      "metadata": {
        "id": "EQ5JA40e1wZV"
      },
      "id": "EQ5JA40e1wZV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "14b956da",
      "metadata": {
        "id": "14b956da"
      },
      "source": [
        "<div class='alert alert-block alert-warning'>\n",
        "            Question:</div>\n",
        "            \n",
        "We improved our initial model with attention. But considering our goal is to **generate text**, we should work on **decoding**. How would you go about implementing that given our current code ? Where is the ideal place to add a decoding function ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class='alert alert-block alert-warning'>\n",
        "            Answer:</div>\n",
        "To improve text generation, we need a more effective decoding strategy beyond simple greedy decoding. Beam Search is a strong alternative because it considers multiple candidate sequences at each step, leading to more fluent and accurate translations.\n",
        "\n",
        "Implementation Approach:\n",
        "\n",
        "- We should modify the current decoding process in the AttentionDecoderRNN model.\n",
        "\n",
        "- Instead of greedy decoding (choosing the highest probability word at each step), we should implement Beam Search to explore multiple translation paths.\n",
        "\n",
        "Ideal Place to Add the Decoding Function:\n",
        "\n",
        "-The best place to add Beam Search is inside the *evaluate()* function, which is responsible for generating output sentences.\n",
        "\n",
        "- This function currently uses greedy decoding by selecting the highest-probability word at each step.\n",
        "\n",
        "- By replacing it with Beam Search, the model will consider multiple translation paths, improving fluency and accuracy."
      ],
      "metadata": {
        "id": "NyCGx63E2411"
      },
      "id": "NyCGx63E2411"
    },
    {
      "cell_type": "markdown",
      "id": "d715ae44",
      "metadata": {
        "id": "d715ae44"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "78018765",
      "metadata": {
        "id": "78018765"
      },
      "source": [
        "Propose a modification of the *relevant function* in this lab in which to include **Beam search**, following the code you used in the previous lab. Similarly as before, compare the BLEU score of:\n",
        "- A model decoding with Beam search\n",
        "- A model using simple greedy decoding\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f387c8f",
      "metadata": {
        "id": "1f387c8f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "def fixed_beam_search_decoder(encoder, decoder, sentence, max_length, input_lang, output_lang, beam_width=6, alpha=0.7):\n",
        "    \"\"\"\n",
        "    Improved Beam Search decoding with better token selection, penalties, and stability.\n",
        "    \"\"\"\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    input_tensor = tensorFromSentence(sentence, input_lang, max_length).view(1, -1).to(device)\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = torch.tensor([[SOS_TOKEN]], dtype=torch.long, device=device)\n",
        "\n",
        "    # Initialize beam with the start token\n",
        "    sequences = [(0.0, [SOS_TOKEN], decoder_hidden)]\n",
        "    input_words = set(input_tensor.squeeze().tolist()) - {PAD_TOKEN, SOS_TOKEN, EOS_TOKEN}\n",
        "\n",
        "    for step in range(max_length):\n",
        "        all_candidates = []\n",
        "\n",
        "        for score, seq, hidden in sequences:\n",
        "            if seq[-1] == EOS_TOKEN:\n",
        "                all_candidates.append((score, seq, hidden))\n",
        "                continue\n",
        "\n",
        "            decoder_input = torch.tensor([[seq[-1]]], dtype=torch.long, device=device)\n",
        "            decoder_output, new_hidden, _ = decoder(encoder_outputs, hidden, 1, decoder_input)\n",
        "\n",
        "            # Dynamic Temperature Scaling for Better Diversity\n",
        "            temperature = 1.3 if step < 3 else 1.0 if step < 8 else 0.9\n",
        "            scaled_logits = decoder_output[0, -1] / temperature\n",
        "            log_probs = torch.log_softmax(scaled_logits, dim=-1)\n",
        "\n",
        "            log_probs, indices = torch.topk(log_probs, beam_width)\n",
        "\n",
        "            for i in range(beam_width):\n",
        "                word_idx = indices[i].item()\n",
        "\n",
        "                # Ensure SOS is never repeated\n",
        "                if step > 0 and word_idx == SOS_TOKEN:\n",
        "                    continue\n",
        "\n",
        "                # Prevent ending too early\n",
        "                if len(seq) < 4 and word_idx == EOS_TOKEN:\n",
        "                    continue\n",
        "\n",
        "                # Improved Length Normalization\n",
        "                new_score = score + (log_probs[i].item() / ((5 + len(seq)) ** alpha))\n",
        "\n",
        "                # More Balanced Penalties\n",
        "                repetition_penalty = 0.3 * seq.count(word_idx)  # Reduce penalty\n",
        "                coverage_penalty = 0.1 * len(set(seq))  # More subtle penalty\n",
        "\n",
        "                new_score -= (repetition_penalty + coverage_penalty)\n",
        "\n",
        "                # Adaptive Generic Word Penalty (Less Aggressive)\n",
        "                if word_idx in {output_lang.word2idx.get(w, -1) for w in [\"tu\", \"il\", \"c'est\", \"on\"]}:\n",
        "                    new_score -= 0.3\n",
        "\n",
        "                # Encourage Input-Aligned Words\n",
        "                if word_idx in input_words and word_idx not in seq:\n",
        "                    new_score += 1.0  # Moderate encouragement\n",
        "\n",
        "                candidate = (new_score, seq + [word_idx], new_hidden.clone().detach())\n",
        "                all_candidates.append(candidate)\n",
        "\n",
        "        # Select Top Beam Sequences\n",
        "        sequences = sorted(all_candidates, key=lambda x: x[0], reverse=True)[:beam_width]\n",
        "\n",
        "    # Convert best sequence into words\n",
        "    best_sequence = sequences[0][1]\n",
        "    decoded_words = []\n",
        "\n",
        "    for idx in best_sequence:\n",
        "        if idx == EOS_TOKEN or idx == SOS_TOKEN:\n",
        "            break\n",
        "        decoded_words.append(output_lang.idx2word.get(idx, \"<UNK>\"))\n",
        "\n",
        "    return \" \".join(decoded_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n",
        "def compute_bleu_beam(encoder, decoder, test_dataloader, dataset, beam_width=3):\n",
        "    \"\"\"\n",
        "    Computes BLEU score for Beam Search translations with debugging.\n",
        "    \"\"\"\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    # Select a small sample of 5 sentences to debug\n",
        "    sample_sentences = 5\n",
        "    count = 0\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        input_tensor, target_tensor = batch\n",
        "\n",
        "        # Convert input sentence from tensor to text\n",
        "        input_sentences = [\n",
        "            dataset.input_lang.idx2word[idx.item()] for idx in input_tensor[0]\n",
        "            if idx.item() not in [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
        "        ]\n",
        "\n",
        "        # Convert target (reference) sentence from tensor to text\n",
        "        target_sentences = [\n",
        "            dataset.output_lang.idx2word[idx.item()] for idx in target_tensor[0]\n",
        "            if idx.item() not in [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
        "        ]\n",
        "\n",
        "        # Get prediction using Beam Search\n",
        "        output_words = beam_search_decoder(\n",
        "            encoder, decoder, \" \".join(input_sentences), dataset.max_length,\n",
        "            dataset.input_lang, dataset.output_lang, beam_width\n",
        "        )\n",
        "\n",
        "        # Lowercase and remove extra spaces\n",
        "        clean_output = [w.lower() for w in output_words.split()]\n",
        "        clean_reference = [[w.lower() for w in target_sentences]]\n",
        "\n",
        "        hypotheses.append(clean_output)\n",
        "        references.append(clean_reference)\n",
        "\n",
        "        # Print the first few translations for debugging\n",
        "        if count < sample_sentences:\n",
        "            print(f\"\\n🔹 **Example {count+1}:**\")\n",
        "            print(f\"📌 Input Sentence: {' '.join(input_sentences)}\")\n",
        "            print(f\"✅ Target Sentence: {' '.join(target_sentences)}\")\n",
        "            print(f\"🚨 Beam Search Output: {' '.join(output_words)}\")\n",
        "            count += 1\n",
        "\n",
        "    # Apply smoothing to avoid zero BLEU scores\n",
        "    smoothie = SmoothingFunction().method1\n",
        "    return corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n"
      ],
      "metadata": {
        "id": "9f47iwfL8I9O"
      },
      "id": "9f47iwfL8I9O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_greedy = compute_bleu(encoder_att, decoder_att, testing_dataloader, filtered_dataset)\n",
        "print(f\"BLEU Score (Greedy Decoding): {bleu_greedy:.4f}\")\n",
        "\n",
        "bleu_beam_fixed = compute_bleu_beam(encoder_att, decoder_att, testing_dataloader, filtered_dataset, beam_width=6)\n",
        "print(f\"BLEU Score (Fixed Beam Search): {bleu_beam_fixed:.4f}\")\n"
      ],
      "metadata": {
        "id": "X2fCBHwp6mb3"
      },
      "id": "X2fCBHwp6mb3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "59ce85d0",
      "metadata": {
        "id": "59ce85d0"
      },
      "source": [
        "Find a model on [Huggingface](https://huggingface.co/tasks/translation) for this task. Try to **understand what model it is** - and who trained it, on which data. Apply it to the same dataset and compute the BLEU score.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# Load pretrained model\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# Translate sentence\n",
        "sentence = \"I am not a doctor but a teacher\"\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True)\n",
        "output_tokens = model.generate(**inputs)\n",
        "translated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Hugging Face Translation:\", translated_text)"
      ],
      "metadata": {
        "id": "CD0RxZZemn7P"
      },
      "id": "CD0RxZZemn7P",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}